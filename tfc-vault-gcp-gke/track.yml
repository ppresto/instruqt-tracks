slug: vault-dr-multi-region-gke
id: 8yyamhy0xjxi
type: track
title: Vault DR - Multi Region GKE Clusters
teaser: |
  Join the devops team on their journey to provision Vault in GCP on GKE.
description: |-
  Work with the devops team as they use Terraform to a fully automate provisioning Vault in GKE. This workshop covers the following topics:

  * Multi Region GKE Vault Deployment
  * Vault Enterprise with Integrated Storage (Raft)
  * Terraform Open Source
  * Disaster Recovery
icon: https://storage.googleapis.com/instruqt-hashicorp-tracks/logo/vault.png
tags:
- terraform
- vault
- gke
- cloud
- enterprise
owner: hashicorp
developers:
- ppresto@hashicorp.com
private: true
published: false
challenges:
- slug: setup-vault-primary
  id: rmbnlqnwyj1r
  type: challenge
  title: "\U0001F947 Set Up Your Primary Vault Cluster"
  teaser: |
    Configure your code editor for Terraform and open a workspace.
  assignment: |-
    Lets get started building our GKE cluster using OSS Terraform with the **gcp-gke** repo that has already been cloned for you.
    ## Create the K8s Cluster with terraform
    Make sure you're in the `us-west-primary` Tab and in dir /root/gcp-gke/us-west-primary. Use vi or the Code Editor Tab to edit **terraform.tfvars**
    to customize your environment.  Then run terraform.
    ```
    terraform init
    terraform apply -auto-approve
    ```
    You should see green output with your GKE cluster information.
    Take a look at the ./main.tf file to see what is being created.
    Terraform is creating our VPC and GKE cluster along with the KMS keys we will use to auto-unseal vault.
    Look at ../templates/override-values-autounseal.yaml to see how vault.yaml is created.
    The helm install uses this file to override defaults.

    ## Visual Studio Code Tour
    While your GKE cluster is being created, open your text editor, Visual Studio Code.
    Open the Code Editor tab on the left. First get familiar with the menus. This is running the Visual Studio Code editor.

    Notice the menu bar with File, Edit, and other menus at the top of the VS Code Editor. You can find all the menus on this menu bar.

    You should see some files in the explorer bar on the left side menu. These are terraform config files to build our Kubernetes Cluster.

    Next you should install the Terraform extension to enable syntax highlighting in your code. Click on the extensions icon - it looks like four small boxes with one slightly out of position.

    Search for `HashiCorp` and select the **\"HashiCorp Terraform 2.x.y\"** extension.
    Click the green **Install** button to install the extension. Then click the **Reload Required** button to activate it.
    Then click the icon with two pages under the File menu so you can see your Terraform file list.
    If you see a popup saying that Terraform 0.x is installed, just close it. We have updated Terraform for you.
    You can check the version by running `terraform version`.  We have enabled auto-save in your Code Editor, so any changes you make will be saved as you type.
    We recommend executing all commands on the \"Shell\" tab. But you can also open and use a terminal window at the bottom
    of the Visual Code Editor by using the Terminal > New Terminal menu or the **<ctrl>-`**
    shortcut.  On Mac its **<cmd> J**.  If you do use the VS Code terminal, you can
    toggle its size up and down with the `^` and inverted `^` buttons above it. You
    can get rid of it with the garbage can and `x` icons.

    ## Time to Install Vault on GKE
    Authenticate and Connect to your new Cluster.
    ```
    ./setkubectl.sh
    ```
    This script will authenticate us to GKE, and create a kms-creds k8s secret with
    our GOOGLE_CREDENTIALS if it doesn't exist.  This is how we support GCP Auto-Unseal.

    With our newly provisioned GKE cluster we can now install Vault using helm.  First add the hashicorp helm repo.
    Next, we'll override the standard helm chart with custom vaules in ./vault.yaml.  This includes
    things like the kms-creds secret needed for auto-unseal.  Use VSCode to take a
    closer look at our customizations.
    ```
    helm repo add hashicorp https://helm.releases.hashicorp.com
    ```
    ```
    helm install vault-primary hashicorp/vault -f vault.yaml -f vault-hc-helm.yaml
    ```

    While the helm chart is installing Vault lets setup an external LB we can use for cross
    GKE cluster replication.  This will give us an external ip that will only route
    traffic to our active node on ports 8200, 8201, 8202.  We have defined selectors
    like `vault-active: "true"` to only target the active vault node to make replication
    work.
    ```
    kubectl apply -f vault-primary-active-lb.yaml
    kubectl get svc vault-primary-active-lb
    ```
    If the external ip is 'pending' just wait and look it up again in a minute.
    At this point vault should be installed. Verify the vault pods are READY and get status
    ```
    kubectl get pods
    kubectl exec vault-primary-0 -- vault status
    ```
    Initialize Vault, join the other raft members, and apply your license by creating /tmp/vault-ent.hclic
    or use the temporary one provided for you. [Use the Install Enterprise-with-raft example here](https://www.vaultproject.io/docs/platform/k8s/helm/examples/enterprise-with-raft) to initialize your cluster, join raft peers, and apply an Enterprise license.  Otherwise, for a quick setup run the command below.
    ```
    ../scripts/init_vault.sh
    ```

    Setup the local Vault CLI Env, and get the Vault URL, and Login token.
    ```
    source ../scripts/getVaultUI.sh vault-primary-ui
    ```
    You should now be running a 3 node Vault Enterprise Cluster in GKE.  Use the VAULT_ADDR and VAULT_TOKEN to login to the vault UI.  You can now setup your policies, auth methods, and secrets engines.
    But wait...

    There is a better way to configure vault then using the UI or CLI.  Why don't we try to use IaC with the [Terraform Vault provider](https://registry.terraform.io/providers/hashicorp/vault/latest/docs).
    ```
    cd ../vault-administration
    terraform init
    terraform apply -auto-approve
    ```
    Now go take a look at your primary cluster's currently configured policies and secrets.

    For a deeper dive into IaC, use the Code Editor tab to review the vault modules and main.tf being used to create a policy and enable kv-v2.
    - `modules/vault-*`
    - `vault-administration/main.tf`

    FYI: We are populating a secret to show case how DR works, but we do not recommend managing sensitive secrets directly in your IaC.
  notes:
  - type: text
    contents: "Welcome to your first day on the job.\n\n<center>\U0001F913 You - Brand
      New Vault Admin\n</center>"
  tabs:
  - title: Code Editor
    type: service
    hostname: workstation
    port: 8443
  - title: us-west-primary
    type: terminal
    hostname: workstation
  - title: GCP Console
    type: service
    hostname: cloud-client
    path: /
    port: 80
  difficulty: basic
  timelimit: 1800
- slug: setup-vault-dr
  id: gvy7iagspn5m
  type: challenge
  title: "\U0001F948 Setup your Secondary Vault DR Cluster"
  teaser: DR is critical for Operations.  Provide the highest SLA's for secrets management
    by setting up Vault's DR replication.
  assignment: |-
    Lets build another GKE cluster in a new region using Terraform. Jump to the `us-central-dr` Tab.

    ## Create the Vault DR Cluster on GKE using terraform
    Make sure you're in the `./us-central-dr`directory.  Use vi or the Code Editor Tab to edit **terraform.tfvars** and customize your environment.  Then run terraform below.
    ```
    terraform init
    terraform apply -auto-approve
    ```
    You should see green output with your GKE cluster information. If you see errors investigate.

    Authenticate and Connect to your new Cluster.  Now you should have 2 GKE clusters in different regions.  Each of these clusters have their own `context`.
    ```
    ./setkubectl.sh
    ```

    Context's allow us to define and manage multiple GKE clusters.  Let's list all our contexts and identify which we are currently using.
    ```
    kubectl config get-contexts -o=name
    kubectl config current-context
    ```
    To switch between contexts and look at our primary GKE cluster use `kubectl config use-context primary`.  Just remember to switch back!

    With our newly provisioned GKE cluster we can now install Vault using helm.
    ```
    helm install vault-dr hashicorp/vault -f vault.yaml
    ```

    While the helm chart is installing Vault lets setup an external LB we can use in the future if we ever need to treat this DR cluster like a primary and setup replication to it.
    ```
    kubectl apply -f vault-dr-active-lb.yaml
    kubectl get svc vault-dr-active-lb
    ```

    Check for the vault Kubernetes pods to be Running and get status
    ```
    kubectl get pods
    kubectl exec vault-dr-0 -- vault status
    ```

    Initialize Vault, join the other raft members, and apply a license by creating /tmp/vault-ent.hclic or use the temporary one.  **Run this script from within the us-centeral-dr directory as shown below.**
    ```
    ../scripts/init_vault.sh
    ```

    Check the health of our Raft peers
    ```
    kubectl exec -ti vault-dr-0 -- \
    vault operator raft list-peers
    ```
    This command requires an active token.  init_vault.sh already logs into vault and sets this for the license update.
    If you need a login token for some reason you can login using the following command: `kubectl exec -ti vault-dr-0 -- vault login $(jq -r '.root_token' < tmp/cluster-keys.json)`

    You can login to the DR clusters UI to confirm its available and empty.
    ```
    ../scripts/getVaultUI.sh vault-dr-ui
    ```
  notes:
  - type: text
    contents: Vault DR Replication is an Enterprise feature.
  tabs:
  - title: Code Editor
    type: service
    hostname: workstation
    port: 8443
  - title: us-central-dr
    type: terminal
    hostname: workstation
  - title: GCP Console
    type: service
    hostname: cloud-client
    path: /
    port: 80
  difficulty: basic
  timelimit: 1800
- slug: setup-vault-replication
  id: gjrada3zvl7o
  type: challenge
  title: "\U0001F5C2 Setup Multi-Region DR Replication"
  teaser: DR is critical for Operations.  Setup Vault's DR replication to provide
    the highest SLA's.
  assignment: |-
    Jump to the `Shell` Tab.  We'll start setting up replication on the Primary cluster.

    ## Setup Vault DR replication
    Go to the `us-west-primary Tab`, change directory and load the primary GKE context.
    ```
    cd ../us-west-primary
    ./setkubectl.sh
    ```

    ### Get External service IP
    To enable Replication on the primary we need the External Service IP to route across GKE clusters.
    ```
    ext_ip=$(kubectl get svc vault-primary-active-lb -o json | jq -r '.status.loadBalancer.ingress[].ip')
    echo "Ext IP: ${ext_ip}"
    ```
    We should see the External IP of the vault-primary-active-lb service.  We are enabling replication on the primary and overriding the local cluster_address with this external endpoint so our DR cluster can establish a connection.

    ### Enable Replication on the Primary
    ```
    kubectl exec -ti vault-primary-0 -- \
    vault write -f sys/replication/dr/primary/enable primary_cluster_addr=https://${ext_ip}:8201
    ```
    The [cluster_address](https://www.vaultproject.io/docs/configuration/listener/tcp#cluster_address) is used for server to server communication.  Whether using TLS or not vault will always encrypt this traffic.  So this URL should always be https.

    ### Create a token for the DR cluster to use
    ```
    kubectl exec -ti vault-primary-0 -- \
    vault write sys/replication/dr/primary/secondary-token id=dr -format=json \
    | tee tmp/secondary-token.json
    ```

    The primary is now setup.  Lets jump back over to the dr cluster and configure replication.
    ```
    cd ../us-central-dr
    ./setkubectl.sh
    ```

    ### Enable Replication with the primary
    ```
    token=$(jq -r '.wrap_info.token' < ../us-west-primary/tmp/secondary-token.json)
    kubectl exec -ti vault-dr-0 -- vault write sys/replication/dr/secondary/enable primary_api_addr=http://${ext_ip}:8200 token=${token}
    ```

    ### Check the DR cluster replication status
    ```
    kubectl --context=dr \
    exec -ti vault-dr-0 -- \
    vault read sys/replication/dr/status
    ```

    ### Check Primary cluster replication status
    ```
    kubectl --context=primary \
    exec -it vault-primary-0 -- \
    vault read sys/replication/dr/status
    ```
    Note:  To quickly get the primary cluster status we are using `--context=primary` in the CLI.
  notes:
  - type: text
    contents: Vault DR Replication is an Enterprise feature.
  tabs:
  - title: Shell
    type: terminal
    hostname: workstation
  - title: Code Editor
    type: service
    hostname: workstation
    port: 8443
  - title: GCP Console
    type: service
    hostname: cloud-client
    path: /
    port: 80
  difficulty: basic
  timelimit: 1800
- slug: promote-dr-to-primary
  id: 62zoapw6kd2y
  type: challenge
  title: "\U0001F9BA  Promote the DR Cluster to Primary"
  teaser: The US West region just suffered a massive outage!  Its time to failover
    and our US Central Vault cluster.
  assignment: |-
    Go to the `Shell` Tab and auth into the DR GKE cluster.
    ```
    cd ../us-central-dr
    ./setkubectl.sh
    ```

    ### Optionally: check the replication status for both DR and Primary
    ```
    kubectl --context=dr \
    exec -ti vault-dr-0 -- \
    vault read sys/replication/dr/status
    ```
    ```
    kubectl --context=primary \
    exec -it vault-primary-0 -- \
    vault read sys/replication/dr/status
    ```

    ### Create DR Operator Batch Token using the new "vault-dr-token" policy.
    This token is required to performa failover, and can be done on the primary cluster anytime.
    ```
    vault_primary_active=$( \
    kubectl --context=primary \
    get pod --selector="vault-active=true" \
    --output=jsonpath={.items..metadata.name})
    ```
    ```
    kubectl --context=primary \
    exec -it ${vault_primary_active} -- \
    vault token create -format=json \
    -orphan -type=batch -policy=vault-dr-token \
    | jq -r '.auth.client_token' | \
    tee /root/gcp-gke/us-west-primary/tmp/dr-batch-token
    ```
    ### Promote Vault DR to Primary
    ```
    dr_batch_token=$(cat /root/gcp-gke/us-west-primary/tmp/dr-batch-token)
    ```
    ```
    vault_dr_active=$( \
    kubectl --context=dr \
    get pod --selector="vault-active=true" \
    --output=jsonpath={.items..metadata.name})
    ```
    ```
    kubectl --context=dr \
    exec -it ${vault_dr_active} -- \
    vault write sys/replication/dr/secondary/promote dr_operation_token=${dr_batch_token}
    ```

    Last, delete the remainder dr pods. Kubernetes will reschedule them and with auto-unseal enabled the DR cluster will now be ready to serve traffic.
    ```
    kubectl --context=dr delete pods -l vault-active=false
    ```

    Login to the DR clusters UI and confirm it has your previous policies and secrets.
    ```
    ../scripts/getVaultUI.sh vault-dr-ui
    ```

    Using the root token that was generated during the creationg of the DR cluster should Fail since it is now a mirror of the Primary.  Try using the token from the primary instead with the following command.
    ```
    jq -r ".root_token" < /root/gcp-gke/us-west-primary/tmp/cluster-keys.json
    ```
  notes:
  - type: text
    contents: Vault DR Replication is an Enterprise feature.
  tabs:
  - title: Shell
    type: terminal
    hostname: workstation
  - title: Code Editor
    type: service
    hostname: workstation
    port: 8443
  - title: GCP Console
    type: service
    hostname: cloud-client
    path: /
    port: 80
  difficulty: basic
  timelimit: 1800
- slug: rebuild-primary-with-new-kms
  id: dstk95a8rtvs
  type: challenge
  title: "\U0001F948 Rebuild a New Primary Cluster"
  teaser: The GCP West Region is back up and its time to rebuild our Main cluster
  assignment: |-
    ## Create the new primary cluster
    Use vi or the Code Editor Tab to review the updated KMS Key values in **terraform.tfvars**
    ```
    cd ../us-west-primary
    cat terraform.tfvars
    grep "new" terraform.tfvars
    ```

    Run terraform to create the new KMS key, and update the custom vault.yaml used in the helm install next.
    ```
    terraform apply -auto-approve
    ```

    Authenticate and Connect to the us-west GKE cluster.
    ```
    ./setkubectl.sh
    ```

    ### Install Vault using helm again, but this time with the updated vault.yaml that contains a new kms key.
    ```
    helm install vault-primary hashicorp/vault -f vault.yaml -f vault-hc-helm.yaml
    ```

    Check for the vault Kubernetes pods to be Running and get status
    ```
    kubectl get pods
    kubectl exec vault-primary-0 -- vault status
    ```

    Initialize Vault, join the other raft members, and apply a license by creating /tmp/vault-ent.hclic or use the temporary one.  **Run this script from within the us-west-primary directory as shown below.**
    ```
    ../scripts/init_vault.sh
    ```

    Check the health of our Raft peers
    ```
    kubectl exec -ti vault-primary-0 -- \
    vault operator raft list-peers
    ```
    This command requires an active login token.

    ### Login to the DR cluster to setup replication to the new primary
    ```
    cd ../us-central-dr
    ./setkubectl.sh
    kubectl exec -ti vault-dr-0 -- vault login $VAULT_TOKEN
    ```

    Get the DR clusters external IP
    ```
    ext_ip=$(kubectl get svc vault-dr-active-lb -o json | jq -r '.status.loadBalancer.ingress[].ip')
    echo "Ext IP: ${ext_ip}"
    ```
    ```
    kubectl exec -ti vault-dr-0 -- \
    vault write -f sys/replication/dr/primary/enable primary_cluster_addr=https://${ext_ip}:8201
    ```

    Create a token for the new cluster to use.  Replication should already be setup on the DR Cluster.
    ```
    kubectl exec -ti vault-dr-0 -- \
    vault write sys/replication/dr/primary/secondary-token id=new-primary -format=json \
    | tee tmp/new-primary-replication-token.json
    ```
    The promoted DR cluster now has everything our new primary cluster will need to enable replication.

    ### Configure the new primary cluster as the DR.  Once replication is complete we will be able to fail back over and use our new primary.  Lets first authenticate.
    ```
    cd ../us-west-primary
    ./setkubectl.sh
    ```

    Replicate with the promoted DR using the token we just created.
    ```
    token=$(jq -r '.wrap_info.token' < ../us-central-dr/tmp/new-primary-replication-token.json)
    kubectl --context=primary exec -ti vault-primary-0 -- vault write sys/replication/dr/secondary/enable primary_api_addr=http://${ext_ip}:8200 token=${token}
    ```

    ### Check DR and Primary cluster replication status
    ```
    kubectl --context=dr \
    exec -ti vault-dr-0 -- \
    vault read sys/replication/dr/status
    ```
    ```
    kubectl --context=primary \
    exec -it vault-primary-0 -- \
    vault read sys/replication/dr/status
    ```

    ### Create DR Operator Batch Token
    This token is required to performa failover, and can be done on the primary cluster anytime.
    ```
    vault_promoteddr_active=$( \
    kubectl --context=dr \
    get pod --selector="vault-active=true" \
    --output=jsonpath={.items..metadata.name})
    ```
    ```
    kubectl --context=dr \
    exec -it ${vault_promoteddr_active} -- \
    vault token create -format=json \
    -orphan -type=batch -policy=vault-dr-token \
    | jq -r '.auth.client_token' | \
    tee /root/gcp-gke/us-central-dr/tmp/promoted-dr-batch-token
    ```
    ### Promote Vault DR to Primary
    ```
    promote_newprimary_batch_token=$(cat /root/gcp-gke/us-central-dr/tmp/promoted-dr-batch-token)
    ```
    ```
    vault_newprimary_active=$( \
    kubectl --context=primary \
    get pod --selector="vault-active=true" \
    --output=jsonpath={.items..metadata.name})
    ```
    ```
    kubectl --context=primary \
    exec -it ${vault_newprimary_active} -- \
    vault write sys/replication/dr/secondary/promote dr_operation_token=${promote_newprimary_batch_token}
    ```

    Last, delete the remainder dr pods. Kubernetes will reschedule them and with auto-unseal enabled the DR cluster will now be ready to serve traffic.
    ```
    kubectl --context=primary delete pods -l vault-active=false
    ```

    Login to the DR clusters UI and confirm it has your previous policies and secrets.
    ```
    ../scripts/getVaultUI.sh vault-primary-ui
    ```

    We want to disable the DR cluster since it's still active and thinks its the primary.
    ```
    kubectl --context=dr \
    exec -it ${vault_promoteddr_active} -- \
    vault write -f sys/replication/dr/primary/disable
    ```
  notes:
  - type: text
    contents: Vault DR Replication is an Enterprise feature.
  tabs:
  - title: Shell
    type: terminal
    hostname: workstation
  - title: Code Editor
    type: service
    hostname: workstation
    port: 8443
  - title: GCP Console
    type: service
    hostname: cloud-client
    path: /
    port: 80
  difficulty: basic
  timelimit: 1800
checksum: "14064968628822891752"
